---
---

@inproceedings{song-etal-2023-uni,
  title     = {Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems},
  author    = {Song, Chiyu  and
               He, Hongliang  and
               Yu, Haofei  and
               Fang, Pengfei  and
               Cui, Leyang  and
               Lan, Zhenzhong},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.388},
  doi       = {10.18653/v1/2023.findings-acl.388},
  pages     = {6231--6244},
  abstract  = {Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates to ensure they are treated equally and design a new attention mechanism to avoid confusion. Our Uni-Encoder can simulate other ranking paradigms using different attention and response concatenation methods. Extensive experiments show that our proposed paradigm achieves new state-of-the-art results on four benchmark datasets with high computational efficiency. For instance, it improves R10@1 by 2.9{\%} with an approximately 4X faster inference speed on the Ubuntu V2 dataset.}
}

@inproceedings{wang-etal-2023-rfid,
  title     = {{RF}i{D}: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering},
  author    = {Wang, Cunxiang  and
               Yu, Haofei  and
               Zhang, Yue},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.155},
  doi       = {10.18653/v1/2023.findings-acl.155},
  pages     = {2473--2481},
  abstract  = {Open-Domain Question Answering (ODQA) systems necessitate a reader model capable of generating answers by simultaneously referring to multiple passages. Although representative models like Fusion-in-Decoder (FiD) have been proposed to address this challenge, these systems can inadvertently rely on spurious features instead of genuine causal relationships between the question and the passages to generate answers. To counter this problem, we introduce the Rational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of FiD to differentiate between causal relationships and spurious features, subsequently guiding the decoder to generate answers informed by this discernment. Experimental results on two ODQA datasets, Natural Questions (NQ) and TriviaQA (TQ), demonstrate that our model surpasses previous methods, achieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and exhibits an enhanced ability to identify causal relationships.}
}

@misc{yu2023trams,
  title         = {TRAMS: Training-free Memory Selection for Long-range Language Modeling},
  author        = {Haofei Yu and Cunxiang Wang and Yue Zhang and Wei Bi},
  year          = {2023},
  eprint        = {2310.15494},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{zhou2023sotopia,
  title   = {SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents},
  author  = {Zhou, Xuhui and Zhu, Hao and Mathur, Leena and Zhang, Ruohong and Yu, Haofei and Qi, Zhengyang and Morency, Louis-Philippe and Bisk, Yonatan and Fried, Daniel and Neubig, Graham and others},
  journal = {arXiv preprint arXiv:2310.11667},
  year    = {2023}
}

@article{weissweiler2023counting,
  title   = {Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model},
  author  = {Weissweiler, Leonie and Hofmann, Valentin and Kantharuban, Anjali and Cai, Anna and Dutt, Ritam and Hengle, Amey and Kabra, Anubha and Kulkarni, Atharva and Vijayakumar, Abhishek and Yu, Haofei and others},
  journal = {arXiv preprint arXiv:2310.15113},
  year    = {2023}
}